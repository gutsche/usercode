#!/usr/bin/env python

import sys, os, getopt
import DBSAPI.dbsApi

def main(argv) :
    """
    
    queryDBS    
    
    query DBS for datasetpath allowing wildcards

    required parameters
    --path <path>                             :       datasetpath allowing wildcards in \"

    optional parameters                       :
    --dbs <dbs>                               :       dbs url (default: http://cmssrv17.fnal.gov:8989/DBS_1_0_5_STABLE/servlet/DBSServlet)
    --global-dbs <dbs>                        :       global dbs (default: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet)
    --se <se>                                 :       storage element url (default: cmssrm.fnal.gov)
    --cff                                     :       write out cff for datasetpaths, outputfilename derived from datasetpath
    --unmerged-cff                            :       write out cff for unmerged datasetpaths, outputfilename derived from datasetpath
    --force                                   :       generate ForceMerge messages
    --twiki                                   :       generate TWiki page, outputfilename derived from CMSSW release
    --publish                                 :       write out publish messages for merged samples not published already
    --crab                                    :       create calls to createCRABReconstructionProject for all merged datasets (requires --project_name_start) and write them in file named after project_name_start
    --project_name_start <name>               :       project name start needed by --crab
    --help (-h)                               :       help
    --debug (-d)                              :       debug statements
    
    """

    # default
    dbs                = 'http://cmssrv17.fnal.gov:8989/DBS_1_0_5_STABLE/servlet/DBSServlet'
    global_dbs         = 'http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet'
    se                 = 'cmssrm.fnal.gov'
    path               = ''
    debug              = 0
    cff                = 0
    unmerged_cff       = 0
    force              = 0
    twiki              = 0
    crab               = 0
    publish            = 0
    project_name_start = ''

    # samples
    samples = ['SingleMuPt1Minus','SingleMuPt1Plus','SingleMuPt2Minus','SingleMuPt2Plus','SingleMuPt5Minus','SingleMuPt5Plus','SingleMuPt10Minus','SingleMuPt10Plus','SingleMuPt25Minus','SingleMuPt25Plus','SingleMuPt100Minus','SingleMuPt100Plus','SingleEPt1Minus','SingleEPt1Plus','SingleEPt2Minus','SingleEPt2Plus','SingleEPt5Minus','SingleEPt5Plus','SingleEPt10Minus','SingleEPt10Plus','SingleEPt25Minus','SingleEPt25Plus','SingleEPt100Minus','SingleEPt100Plus','SinglePiPt1Minus','SinglePiPt1Plus','SinglePiPt2Minus','SinglePiPt2Plus','SinglePiPt5Minus','SinglePiPt5Plus','SinglePiPt10Minus','SinglePiPt10Plus','SinglePiPt25Minus','SinglePiPt25Plus','SinglePiPt100Minus','SinglePiPt100Plus','Singlegamma_pt_5','Singlegamma_pt_10','Singlegamma_pt_55','Singlegamma_pt_100','MinBias','ZMuMu','ZMuMu_LowLumiPileup','BJets120_170','BJets120_170_LowLumiPileup','TTBar','TTBar_LowLumiPileup']


    try:
        cmssw_version = os.environ.get("CMSSW_VERSION")
    except:
        print ''
        print 'CMSSW version cannot be determined from $CMSSW_VERSION, please setup a user area!'
        sys.exit(2)

    if cmssw_version == None:
        print ''
        print 'CMSSW version cannot be determined from $CMSSW_VERSION, please setup a user area!'
        sys.exit(2)

    try:
        opts, args = getopt.getopt(argv, "", ["help", "debug", "path=", "dbs=", "cff", "force", "twiki", "se=", "crab", "project_name_start=","publish","global-dbs="])
    except getopt.GetoptError:
        print main.__doc__
        sys.exit(2)

    # check command line parameter
    for opt, arg in opts :
        if opt == "--help" :
            print main.__doc__
            sys.exit()
        elif opt == "--debug" :
            debug = 1
        elif opt == "--dbs" :
            dbs = arg
        elif opt == "--global-dbs" :
            global_dbs = arg
        elif opt == "--se" :
            se = arg
        elif opt == "--path" :
            path = arg
        elif opt == "--cff" :
            cff = 1
        elif opt == "--unmerged-cff" :
            unmerged_cff = 1
        elif opt == "--force" :
            force = 1
        elif opt == "--twiki" :
            twiki = 1
        elif opt == "--crab" :
            crab = 1
        elif opt == "--publish" :
            publish = 1
        elif opt == "--project_name_start" :
            project_name_start = arg
            

    if path == '' :
        print main.__doc__
        sys.exit(2)

    if crab == 1 and project_name_start == '' :
        print main.__doc__
        sys.exit(2)

    if force == 0 and publish == 1:
        print ''
        print 'To avoid publishing not completely merged datasets, please set --force AND --publish'
        print main.__doc__
        sys.exit(2)

    if debug:
        print 'Parameters:'
        print 'path=',path
        print 'dbs=',dbs

        

    # init dbs
    args = {}
    args['url']   = dbs
    args['level'] = 'CRITICAL'

    try:
        api = DBSAPI.dbsApi.DbsApi(args)
        if debug:
            print ''
            print 'Connected dbs:',dbs
    except:
        print ''
        print 'Problem connecting DBS'
        sys.exit(1)

    # arrays
    unmergedDataSets      = {}
    dataSets              = {}

    # list processed datasets
    try:
        path_array = path.split('/')
    except:
        print ''
        print 'Datasetpath:',path,'cannot be parsed by /PRIMARY/PROCESSED/TIER'
        print 'Default to /*/*/*'
        path_array = ['','*','*','*']

    if debug:
        print ''
        print 'List processed datasets for:',path_array[1:]
        
    datasets   = api.listProcessedDatasets(path_array[1],path_array[3],path_array[2])

    for dataset in datasets:
        for datasetpath in dataset.get('PathList') :
            if datasetpath.find('unmerged') > 0 :
                temp = queryDataset(api,datasetpath,se,debug)
                unmergedDataSets[temp['datasetpath'].split('/')[1].replace('DevelopmentSample','')] = temp
            else :
                temp = queryDataset(api,datasetpath,se,debug)
                dataSets[temp['datasetpath'].split('/')[1].replace('DevelopmentSample','')] = temp

    if len(unmergedDataSets) > 0 :
        print ''
        print 'Unmerged DataSets: available:',len(unmergedDataSets)
        print ''
        for sample in samples:
            if sample in unmergedDataSets.keys() :
                print "%s: events:%6d size: %s GB" % (unmergedDataSets[sample]['datasetpath'].ljust(115),unmergedDataSets[sample]['events'],formatSize(unmergedDataSets[sample]['size'],True))
                if unmerged_cff == 1 :
                    writeCff(unmergedDataSets[sample],debug)
        for dataset in unmergedDataSets.keys():
            if unmergedDataSets[dataset]['datasetpath'].split('/')[1].replace('DevelopmentSample','') not in samples:
                print "%s: events:%6d size: %s GB" % (unmergedDataSets[dataset]['datasetpath'].ljust(115),unmergedDataSets[dataset]['events'],formatSize(unmergedDataSets[dataset]['size'],True))
                if unmerged_cff == 1 :
                    writeCff(unmergedDataSets[dataset],debug)

    if len(dataSets) > 0 :
        print ''
        print 'DataSets: available:',len(dataSets)
        print ''
        for sample in samples:
            if sample in dataSets.keys() :
                print "%s: events: %6d size: %s GB" % (dataSets[sample]['datasetpath'].ljust(115),dataSets[sample]['events'],formatSize(dataSets[sample]['size'],True))
                if cff == 1 :
                    writeCff(dataSets[sample],debug)
        for dataset in dataSets.keys():
            if dataSets[dataset]['datasetpath'].split('/')[1].replace('DevelopmentSample','') not in samples:
                print "%s: events: %6d size: %s GB" % (dataSets[dataset]['datasetpath'].ljust(115),dataSets[dataset]['events'],formatSize(dataSets[dataset]['size'],True))
                if cff == 1 :
                    writeCff(dataSets[dataset],debug)

    if force == 1:
        print ''
        print 'Samples which have to be ForceMerge\'d'
        print ''
        for merged in dataSets.keys():
            for unmerged in unmergedDataSets.keys():
                if dataSets[merged]['datasetpath'] == unmergedDataSets[unmerged]['datasetpath'].replace('-unmerged','') :
                    if dataSets[merged]['events'] != unmergedDataSets[unmerged]['events'] :
                        print 'python2.4 publish.py ForceMerge',unmergedDataSets[unmerged]['datasetpath']
        for sample in samples :
            if sample in unmergedDataSets.keys() and sample not in dataSets.keys():
                print 'python2.4 publish.py ForceMerge',unmergedDataSets[unmerged]['datasetpath']

    if twiki == 1:
        row_starts = {}
        row_starts['SingleMuPt1Minus'] = '|single &mu; minus p<sub>T</sub> 1 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_1_minus.cff][cff]]  |'
        row_starts['SingleMuPt1Plus'] = '|single &mu; plus p<sub>T</sub> 1 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_1_plus.cff][cff]]  |'
        row_starts['SingleMuPt2Minus'] = '|single &mu; minus p<sub>T</sub> 2 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_2_minus.cff][cff]]  |'
        row_starts['SingleMuPt2Plus'] = '|single &mu; plus p<sub>T</sub> 2 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_2_plus.cff][cff]]  |'
        row_starts['SingleMuPt5Minus'] = '|single &mu; minus p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_5_minus.cff][cff]]  |'
        row_starts['SingleMuPt5Plus'] = '|single &mu; plus p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_5_plus.cff][cff]]  |'
        row_starts['SingleMuPt10Minus'] = '|single &mu; minus p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_10_minus.cff][cff]]  |'
        row_starts['SingleMuPt10Plus'] = '|single &mu; plus p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_10_plus.cff][cff]]  |'
        row_starts['SingleMuPt25Minus'] = '|single &mu; minus p<sub>T</sub> 25 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_25_minus.cff][cff]]  |'
        row_starts['SingleMuPt25Plus'] = '|single &mu; plus p<sub>T</sub> 25 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_25_plus.cff][cff]]  |'
        row_starts['SingleMuPt100Minus'] = '|single &mu; minus p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_100_minus.cff][cff]]  |'
        row_starts['SingleMuPt100Plus'] = '|single &mu; plus p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_mu_pt_100_plus.cff][cff]]  |'
        row_starts['SingleEPt1Minus'] = '|single e minus p<sub>T</sub> 1 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_1_minus.cff][cff]]  |'
        row_starts['SingleEPt1Plus'] = '|single e plus p<sub>T</sub> 1 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_1_plus.cff][cff]]  |'
        row_starts['SingleEPt2Minus'] = '|single e minus p<sub>T</sub> 2 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_2_minus.cff][cff]]  |'
        row_starts['SingleEPt2Plus'] = '|single e plus p<sub>T</sub> 2 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_2_plus.cff][cff]]  |'
        row_starts['SingleEPt5Minus'] = '|single e minus p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_5_minus.cff][cff]]  |'
        row_starts['SingleEPt5Plus'] = '|single e plus p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_5_plus.cff][cff]]  |'
        row_starts['SingleEPt10Minus'] = '|single e minus p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_10_minus.cff][cff]]  |'
        row_starts['SingleEPt10Plus'] = '|single e plus p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_10_plus.cff][cff]]  |'
        row_starts['SingleEPt25Minus'] = '|single e minus p<sub>T</sub> 25 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_25_minus.cff][cff]]  |'
        row_starts['SingleEPt25Plus'] = '|single e plus p<sub>T</sub> 25 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_25_plus.cff][cff]]  |'
        row_starts['SingleEPt100Minus'] = '|single e minus p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_100_minus.cff][cff]]  |'
        row_starts['SingleEPt100Plus'] = '|single e plus p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_e_pt_100_plus.cff][cff]]  |'
        row_starts['SinglePiPt1Minus'] = '|single &pi; minus p<sub>T</sub> 1 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_1_minus.cff][cff]]  |'
        row_starts['SinglePiPt1Plus'] = '|single &pi; plus p<sub>T</sub> 1 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_1_plus.cff][cff]]  |'
        row_starts['SinglePiPt2Minus'] = '|single &pi; minus p<sub>T</sub> 2 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_2_minus.cff][cff]]  |'
        row_starts['SinglePiPt2Plus'] = '|single &pi; plus p<sub>T</sub> 2 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_2_plus.cff][cff]]  |'
        row_starts['SinglePiPt5Minus'] = '|single &pi; minus p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_5_minus.cff][cff]]  |'
        row_starts['SinglePiPt5Plus'] = '|single &pi; plus p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_5_plus.cff][cff]]  |'
        row_starts['SinglePiPt10Minus'] = '|single &pi; minus p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_10_minus.cff][cff]]  |'
        row_starts['SinglePiPt10Plus'] = '|single &pi; plus p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_10_plus.cff][cff]]  |'
        row_starts['SinglePiPt25Minus'] = '|single &pi; minus p<sub>T</sub> 25 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_25_minus.cff][cff]]  |'
        row_starts['SinglePiPt25Plus'] = '|single &pi; plus p<sub>T</sub> 25 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_25_plus.cff][cff]]  |'
        row_starts['SinglePiPt100Minus'] = '|single &pi; minus p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_100_minus.cff][cff]]  |'
        row_starts['SinglePiPt100Plus'] = '|single &pi; plus p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_pi_pt_100_plus.cff][cff]]  |'
        row_starts['Singlegamma_pt_5'] = '|single &gamma; p<sub>T</sub> 5 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_gamma_pt_5.cff][cff]]  |'
        row_starts['Singlegamma_pt_10'] = '|single &gamma; p<sub>T</sub> 10 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_gamma_pt_10.cff][cff]]  |'
        row_starts['Singlegamma_pt_55'] = '|single &gamma; p<sub>T</sub> 55 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_gamma_pt_55.cff][cff]]  |'
        row_starts['Singlegamma_pt_100'] = '|single &gamma; p<sub>T</sub> 100 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/single_gamma_pt_100.cff][cff]]  |'
        row_starts['MinBias'] = '|minbias|  [[%PUBURLPATH%/%WEB%/RoadSearch/minbias.cff][cff]]  |'
        row_starts['ZMuMu'] = '|Z->&mu;&mu;|  [[%PUBURLPATH%/%WEB%/RoadSearch/zmumu.cff][cff]]  |'
        row_starts['ZMuMu_LowLumiPileup'] = '|Z->&mu;&mu; low-lumi pileup|  [[%PUBURLPATH%/%WEB%/RoadSearch/zmumu.cff][cff]]  |'
        row_starts['BJets120_170'] = '|b<sub>Jets</sub> 120 - 170 !GeV|  [[%PUBURLPATH%/%WEB%/RoadSearch/bjets_120_170.cff][cff]]  |'
        row_starts['BJets120_170_LowLumiPileup'] = '|b<sub>Jets</sub> 120 - 170 !GeV low-lumi pileup|  [[%PUBURLPATH%/%WEB%/RoadSearch/bjets_120_170.cff][cff]]  |'
        row_starts['TTBar'] = '|TTbar|  [[%PUBURLPATH%/%WEB%/RoadSearch/ttbar.cff][cff]]  |'
        row_starts['TTBar_LowLumiPileup'] = '|TTbar low-lumi pileup|  [[%PUBURLPATH%/%WEB%/RoadSearch/ttbar.cff][cff]]  |'

        twikifile = open(cmssw_version + '.twiki','w')
        twikifile.write('---+ Development Samples for ' + cmssw_version + '\n')
        twikifile.write('\n')
        twikifile.write('*dbs_url = ' + dbs + '*\n')
        twikifile.write('\n')
        twikifile.write('|  *Sample*  |  *Generation cff*  |  *DatasetPath*  |  *Number of events*  |  *Size [GB]*  |  *Files at FNAL (cff)*  |\n')
        for sample in samples:
            if sample in dataSets.keys() :
                twikifile.write(row_starts[sample] + dataSets[sample]['datasetpath'] + '|  ' + str(dataSets[sample]['events']) + ' |  ' + formatSize(dataSets[sample]['size']) + ' |  ' + '[[%ATTACHURL%/' + formatCFFName(dataSets[sample]['datasetpath']) + '][cff]]  |\n')
            else :
                twikifile.write(row_starts[sample] + ' | | | |\n')
        twikifile.close()

    if crab == 1:
        project_name_extension = {}
        project_name_extension['SingleMuPt1Minus']           = 10
        project_name_extension['SingleMuPt1Plus']            = 20
        project_name_extension['SingleMuPt2Minus']           = 11
        project_name_extension['SingleMuPt2Plus']            = 21
        project_name_extension['SingleMuPt5Minus']           = 12
        project_name_extension['SingleMuPt5Plus']            = 22
        project_name_extension['SingleMuPt10Minus']          = 13
        project_name_extension['SingleMuPt10Plus']           = 23
        project_name_extension['SingleMuPt25Minus']          = 14
        project_name_extension['SingleMuPt25Plus']           = 24
        project_name_extension['SingleMuPt100Minus']         = 15
        project_name_extension['SingleMuPt100Plus']          = 25
        project_name_extension['SingleEPt1Minus']            = 30
        project_name_extension['SingleEPt1Plus']             = 40
        project_name_extension['SingleEPt2Minus']            = 31
        project_name_extension['SingleEPt2Plus']             = 41
        project_name_extension['SingleEPt5Minus']            = 32
        project_name_extension['SingleEPt5Plus']             = 42
        project_name_extension['SingleEPt10Minus']           = 33
        project_name_extension['SingleEPt10Plus']            = 43
        project_name_extension['SingleEPt25Minus']           = 34
        project_name_extension['SingleEPt25Plus']            = 44
        project_name_extension['SingleEPt100Minus']          = 35
        project_name_extension['SingleEPt100Plus']           = 45
        project_name_extension['SinglePiPt1Minus']           = 50
        project_name_extension['SinglePiPt1Plus']            = 60
        project_name_extension['SinglePiPt2Minus']           = 51
        project_name_extension['SinglePiPt2Plus']            = 61
        project_name_extension['SinglePiPt5Minus']           = 52
        project_name_extension['SinglePiPt5Plus']            = 62
        project_name_extension['SinglePiPt10Minus']          = 53
        project_name_extension['SinglePiPt10Plus']           = 63
        project_name_extension['SinglePiPt25Minus']          = 54
        project_name_extension['SinglePiPt25Plus']           = 64
        project_name_extension['SinglePiPt100Minus']         = 55
        project_name_extension['SinglePiPt100Plus']          = 65
        project_name_extension['Singlegamma_pt_5']           = 70
        project_name_extension['Singlegamma_pt_10']          = 71
        project_name_extension['Singlegamma_pt_55']          = 72
        project_name_extension['Singlegamma_pt_100']         = 73
        project_name_extension['MinBias']                    = 80
        project_name_extension['ZMuMu']                      = 81
        project_name_extension['ZMuMu_LowLumiPileup']        = 82
        project_name_extension['BJets120_170']               = 83
        project_name_extension['BJets120_170_LowLumiPileup'] = 84
        project_name_extension['TTBar']                      = 85
        project_name_extension['TTBar_LowLumiPileup']        = 86

        events = {}
        events['SingleMuPt1Minus']           = 1000
        events['SingleMuPt1Plus']            = 1000
        events['SingleMuPt2Minus']           = 1000
        events['SingleMuPt2Plus']            = 1000
        events['SingleMuPt5Minus']           = 1000
        events['SingleMuPt5Plus']            = 1000
        events['SingleMuPt10Minus']          = 1000
        events['SingleMuPt10Plus']           = 1000
        events['SingleMuPt25Minus']          = 1000
        events['SingleMuPt25Plus']           = 1000
        events['SingleMuPt100Minus']         = 1000
        events['SingleMuPt100Plus']          = 1000
        events['SingleEPt1Minus']            = 1000
        events['SingleEPt1Plus']             = 1000
        events['SingleEPt2Minus']            = 1000
        events['SingleEPt2Plus']             = 1000
        events['SingleEPt5Minus']            = 1000
        events['SingleEPt5Plus']             = 1000
        events['SingleEPt10Minus']           = 1000
        events['SingleEPt10Plus']            = 1000
        events['SingleEPt25Minus']           = 1000
        events['SingleEPt25Plus']            = 1000
        events['SingleEPt100Minus']          = 1000
        events['SingleEPt100Plus']           = 1000
        events['SinglePiPt1Minus']           = 1000
        events['SinglePiPt1Plus']            = 1000
        events['SinglePiPt2Minus']           = 1000
        events['SinglePiPt2Plus']            = 1000
        events['SinglePiPt5Minus']           = 1000
        events['SinglePiPt5Plus']            = 1000
        events['SinglePiPt10Minus']          = 1000
        events['SinglePiPt10Plus']           = 1000
        events['SinglePiPt25Minus']          = 1000
        events['SinglePiPt25Plus']           = 1000
        events['SinglePiPt100Minus']         = 1000
        events['SinglePiPt100Plus']          = 1000
        events['Singlegamma_pt_5']           = 1000
        events['Singlegamma_pt_10']          = 1000
        events['Singlegamma_pt_55']          = 1000
        events['Singlegamma_pt_100']         = 1000
        events['MinBias']                    = 500
        events['ZMuMu']                      = 200
        events['ZMuMu_LowLumiPileup']        = 200
        events['BJets120_170']               = 200
        events['BJets120_170_LowLumiPileup'] = 200
        events['TTBar']                      = 200
        events['TTBar_LowLumiPileup']        = 200


        crabfile = open(project_name_start + '.crab','w')
        # change script permission to executable
        os.chmod(project_name_start + '.crab',0755)
        # write header
        crabfile.write('#!/bin/bash\n')
        
        print ''
        print 'writing out crab project creation commands to file: ' + project_name_start + '.crab'

        for sample in samples :
            if sample in dataSets.keys():
                msg  = './createCRABReconstructionProject --project_name ' + project_name_start + '_' + str(project_name_extension[sample])
                msg += ' --datasetpath ' + dataSets[sample]['datasetpath']
                msg += ' --events ' + str(events[sample])
                msg += ' --dbs ' + dbs
                msg += '\n'
                crabfile.write(msg)
        counter = 90
        for merged in dataSets.keys():
            if merged not in samples:
                msg  = './createCRABReconstructionProject --project_name ' + project_name_start + '_' + str(counter)
                counter += 1
                msg += ' --datasetpath ' + dataSets[sample]['datasetpath']
                msg += ' --events 1000'
                msg += '\n'
                crabfile.write(msg)

    if publish == 1 :

        # init global dbs
        global_args = {}
        global_args['url']   = global_dbs
        global_args['level'] = 'CRITICAL'
        try:
            global_api = DBSAPI.dbsApi.DbsApi(global_args)
        except:
            print ''
            print 'Problem connecting to DBS:',global_dbs
            sys.exit(1)

        print ''
        print 'following datasets stil have to be published to following DBS instance:',global_dbs
        print ''

        for sample in samples:
            if sample in dataSets.keys() :
                published_dataset = queryDataset(global_api,dataSets[sample]['datasetpath'],se,debug)
                if 'datasetpath' not in published_dataset.keys():
                    print 'python2.4 publish.py DBSInterface:MigrateDatasetToGlobal',dataSets[sample]['datasetpath']
        for dataset in dataSets.keys():
            if dataset not in samples :
                published_dataset = queryDataset(global_api,dataSets[dataset]['datasetpath'],se,debug)
                if 'datasetpath' not in published_dataset.keys():
                    print 'python2.4 publish.py DBSInterface:MigrateDatasetToGlobal',dataSets[dataset]['datasetpath']

def formatSize(size,format = False) :
    """
    format size
    """
    if format :
        result = "%5.2f" % (size/1024.0/1024.0/1024.0)
    else :
        result = "%.2f" % (size/1024.0/1024.0/1024.0)
    return result

def formatCFFName(datasetpath) :
    """
    format cff filename
    """
    result = datasetpath.replace('/','__')[2:]+'.cff'
    return result

def queryDataset(api,datasetpath,se,debug):
    """
    query dataset
    """

    result    = {}
    events    = 0
    size      = 0
    filenames = []

    try:
        blocks = api.listBlocks(dataset=datasetpath,block_name="*",storage_element_name=se);
    except:
        blocks = []
    for block in blocks:
        files = api.listFiles(blockName=block['Name'])
        for file in files:
            filenames.append(file['LogicalFileName'])
            events += file['NumberOfEvents']
            size   += file['FileSize']
            if debug:
                print file['LogicalFileName'],file['NumberOfEvents'],file['FileSize']

    if len(blocks) > 0 :
        result['datasetpath'] = datasetpath
        result['events']      = events
        result['size']        = size
        result['filenames']   = filenames

    if debug:
        print result['datasetpath'],result['events'],result['size']

    return result

def writeCff(dataset,cff):
    """
    write cff
    """

    outputfile = file(formatCFFName(dataset['datasetpath']),'w')

    output = ''
    output += '#\n'
    output += '# datasetpath: '+ str(dataset['datasetpath']) + '\n'
    output += '# events     : '+ str(dataset['events']) + '\n'
    output += '# size       : %s GB\n' % (formatSize(dataset['size'],True))
    output += '#\n'
    output += 'replace PoolSource.fileNames = {\n'
    for filename in dataset['filenames'] :
        output += '  "' + filename + '",\n'
    output = output[:-2]+'\n'
    output += '}\n'

    outputfile.write(output)

    outputfile.close()

if __name__ == '__main__' :
    main(sys.argv[1:])

