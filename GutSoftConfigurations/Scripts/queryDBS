#!/usr/bin/env python2.4

import sys, os, getopt
import DBSAPI.dbsApi

def main(argv) :
    """
    
    queryDBS    
    
    query DBS for datasetpath allowing wildcards

    required parameters
    --path <path>                             :       datasetpath allowing wildcards in \"

    optional parameters                       :
    --dbs <dbs>                               :       dbs url (default: http://cmssrv17.fnal.gov:8989/DBS_1_0_5_STABLE/servlet/DBSServlet)
    --global-dbs <dbs>                        :       global dbs (default: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet)
    --se <se>                                 :       storage element url (default: cmssrm.fnal.gov)
    --cff                                     :       write out cff for datasetpaths, outputfilename derived from datasetpath
    --unmerged-cff                            :       write out cff for unmerged datasetpaths, outputfilename derived from datasetpath
    --force                                   :       generate ForceMerge messages
    --publish                                 :       write out publish messages for merged samples not published already, requires --force
    --publish-anyway                          :       write out publish messages for merged samples not published already without requiring --force
    --check                                   :       prepare scripts to check if files of datasets have been written to dCache
    --check-unmerged                          :       prepare scripts to check if files of unmerged datasets have been written to dCache
    --help (-h)                               :       help
    --debug (-d)                              :       debug statements
    
    """

    # default
    dbs                = 'http://cmssrv46.fnal.gov:8080/DBS/servlet/DBSServlet'
    global_dbs         = 'http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet'
    se                 = 'cmssrm.fnal.gov'
    path               = ''
    debug              = 0
    cff                = 0
    unmerged_cff       = 0
    force              = 0
    publish            = 0
    publish_anyway     = 0
    check              = 0
    check_unmerged     = 0

    try:
        opts, args = getopt.getopt(argv, "", ["help", "debug", "path=", "dbs=", "cff", "force", "se=", "publish","publish-anyway", "global-dbs=", "check", "check-unmerged", "unmerged-cff"])
    except getopt.GetoptError:
        print main.__doc__
        sys.exit(2)

    # check command line parameter
    for opt, arg in opts :
        if opt == "--help" :
            print main.__doc__
            sys.exit()
        elif opt == "--debug" :
            debug = 1
        elif opt == "--dbs" :
            dbs = arg
        elif opt == "--global-dbs" :
            global_dbs = arg
        elif opt == "--se" :
            se = arg
        elif opt == "--path" :
            path = arg
        elif opt == "--cff" :
            cff = 1
        elif opt == "--unmerged-cff" :
            unmerged_cff = 1
        elif opt == "--force" :
            force = 1
        elif opt == "--publish" :
            publish = 1
        elif opt == "--publish-anyway" :
            publish_anyway = 1
        elif opt == "--check" :
            check = 1
        elif opt == "--check-unmerged" :
            check_unmerged = 1
            
    # samples
    samples = []
    try:
        cmssw_version = os.environ.get("CMSSW_VERSION")
    except:
        print ''
        print 'CMSSW version cannot be determined from $CMSSW_VERSION, please setup a user area!'
        sys.exit(2)

    if cmssw_version == None:
        print ''
        print 'CMSSW version cannot be determined from $CMSSW_VERSION, please setup a user area!'
        sys.exit(2)

        
    if path == '' :
        print main.__doc__
        sys.exit(2)

    if force == 0 and publish == 1:
        print ''
        print 'To avoid publishing not completely merged datasets, please set --force AND --publish'
        print main.__doc__
        sys.exit(2)

    if debug:
        print 'Parameters:'
        print 'path=',path
        print 'dbs=',dbs

        

    # init dbs
    args = {}
    args['url']   = dbs
    args['level'] = 'CRITICAL'

    try:
        api = DBSAPI.dbsApi.DbsApi(args)
        if debug:
            print ''
            print 'Connected dbs:',dbs
    except:
        print ''
        print 'Problem connecting DBS'
        sys.exit(1)

    # arrays
    unmergedDataSets      = {}
    dataSets              = {}

    # list processed datasets
    try:
        path_array = path.split('/')
    except:
        print ''
        print 'Datasetpath:',path,'cannot be parsed by /PRIMARY/PROCESSED/TIER'
        print 'Default to /*/*/*'
        path_array = ['','*','*','*']

    if debug:
        print ''
        print 'List processed datasets for:',path_array[1:]
        
    datasets   = api.listProcessedDatasets(path_array[1],path_array[3],path_array[2])

    for dataset in datasets:
        for datasetpath in dataset.get('PathList') :
            temp = queryDataset(api,datasetpath,se,debug)
            if len(temp.keys()) > 0:
                if datasetpath.find('unmerged') >= 0 :
                    unmergedDataSets[temp['datasetpath']] = temp
                else :
                    dataSets[temp['datasetpath']] = temp

    if len(unmergedDataSets) > 0 :
        print ''
        print 'Unmerged DataSets in',dbs,':',len(unmergedDataSets)
        print ''
        for dataset in unmergedDataSets.keys():
            print "%s: events: %8d size: %s GB" % (unmergedDataSets[dataset]['datasetpath'].ljust(115),unmergedDataSets[dataset]['events'],formatSize(unmergedDataSets[dataset]['size'],True))
            if unmerged_cff == 1 :
                writeCff(unmergedDataSets[dataset],debug)
            if check_unmerged == 1:
                writeCheckScript(unmergedDataSets[dataset],debug)

    if len(dataSets) > 0 :
        print ''
        print 'DataSets in',dbs,':',len(dataSets)
        print ''
        for dataset in dataSets.keys():
            print "%s: events: %8d size: %s GB" % (dataSets[dataset]['datasetpath'].ljust(115),dataSets[dataset]['events'],formatSize(dataSets[dataset]['size'],True))
            if cff == 1 :
                writeCff(dataSets[dataset],debug)
            if check == 1:
                writeCheckScript(dataSets[dataset],debug)

    do_force = []

    if force == 1:
        print ''
        print 'Samples which have to be ForceMerge\'d'
        print ''
        for merged in dataSets.keys():
            for unmerged in unmergedDataSets.keys():
                if dataSets[merged]['datasetpath'] == unmergedDataSets[unmerged]['datasetpath'].replace('-unmerged','') :
                    if dataSets[merged]['events'] < unmergedDataSets[unmerged]['events'] :
                        do_force.append(dataSets[merged]['datasetpath'])
                        print 'python2.4 publish.py ForceMerge',unmergedDataSets[unmerged]['datasetpath']

    if publish == 1 or publish_anyway == 1:

        # init global dbs
        global_args = {}
        global_args['url']   = global_dbs
        global_args['level'] = 'CRITICAL'
        try:
            global_api = DBSAPI.dbsApi.DbsApi(global_args)
        except:
            print ''
            print 'Problem connecting to DBS:',global_dbs
            sys.exit(1)

        print ''
        print 'following datasets still have to be published to following DBS instance:',global_dbs
        print ''

        for dataset in dataSets.keys():
            published_dataset = queryDataset(global_api,dataSets[dataset]['datasetpath'],se,debug)
            if 'datasetpath' not in published_dataset.keys():
                if dataSets[dataset]['datasetpath'] not in do_force :
                    print 'python2.4 publish.py DBSInterface:MigrateDatasetToGlobal',dataSets[dataset]['datasetpath']
                    print 'python2.4 publish.py PhEDExInjectDataset',dataSets[dataset]['datasetpath']
            elif published_dataset['events'] != dataSets[dataset]['events'] :
                if dataSets[dataset]['datasetpath'] not in do_force :
                    print 'python2.4 publish.py DBSInterface:MigrateDatasetToGlobal',dataSets[dataset]['datasetpath']
                    print 'python2.4 publish.py PhEDExInjectDataset',dataSets[dataset]['datasetpath']

def formatSize(size,format = False) :
    """
    format size
    """
    if format :
        result = "%7.2f" % (size/1024.0/1024.0/1024.0)
    else :
        result = "%.2f" % (size/1024.0/1024.0/1024.0)
    return result

def formatCFFName(datasetpath) :
    """
    format cff filename
    """
    result = datasetpath.replace('/','__')[2:]+'.cff'
    return result

def queryDataset(api,datasetpath,se,debug):
    """
    query dataset
    """

    result    = {}
    events    = 0
    size      = 0
    filenames = []

    try:
        blocks = api.listBlocks(dataset=datasetpath,block_name="*",storage_element_name=se);
    except:
        blocks = []
    for block in blocks:
        files = api.listFiles(blockName=block['Name'])
        for file in files:
            filenames.append(file['LogicalFileName'])
            events += file['NumberOfEvents']
            size   += file['FileSize']
            if debug:
                print file['LogicalFileName'],file['NumberOfEvents'],file['FileSize']

    if len(blocks) > 0 :
        result['datasetpath'] = datasetpath
        result['events']      = events
        result['size']        = size
        result['filenames']   = filenames

    if debug:
        print result['datasetpath'],result['events'],result['size']

    return result

def writeCff(dataset,debug):
    """
    write cff
    """

    outputfile = file(formatCFFName(dataset['datasetpath']),'w')

    output = ''
    output += '#\n'
    output += '# datasetpath: '+ str(dataset['datasetpath']) + '\n'
    output += '# events     : '+ str(dataset['events']) + '\n'
    output += '# size       : %s GB\n' % (formatSize(dataset['size'],True))
    output += '#\n'
    output += 'replace PoolSource.fileNames = {\n'
    for filename in dataset['filenames'] :
        output += '  "' + filename + '",\n'
    output = output[:-2]+'\n'
    output += '}\n'

    outputfile.write(output)

    outputfile.close()

def writeCheckScript(dataset,debug):
    """
    write cff
    """

    outputfile_name = 'check_' + formatCFFName(dataset['datasetpath']).replace('.cff','.sh')

    outputfile = file(outputfile_name,'w')

    output = '#!/bin/bash\n'
    for filename in dataset['filenames'] :
        output += '\n'
        output += 'if [ ! -f /pnfs/cms/WAX/11' + filename + ' ]; then\n'
        output += '   echo "' + filename + '"\n'
        output += 'fi\n'

    outputfile.write(output)

    os.chmod(outputfile_name,0755)

    outputfile.close()

if __name__ == '__main__' :
    main(sys.argv[1:])

